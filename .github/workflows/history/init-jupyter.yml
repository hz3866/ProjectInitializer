name: Init Jupyter Project

on:
  workflow_dispatch:
    inputs:
      project_name:
        description: 'Project Name (will be used as new repository name)'
        required: true
        type: string

      python_version:
        description: 'Python Version'
        required: true
        type: choice
        options:
          - '3.10'
          - '3.11'
          - '3.12'
        default: '3.11'

      repo_visibility:
        description: 'Repository Visibility'
        required: true
        type: choice
        options:
          - 'public'
          - 'private'
        default: 'public'

jobs:
  create-jupyter:
    runs-on: ubuntu-latest
    steps:
      - name: Validate project name
        run: |
          if [[ ! "${{ inputs.project_name }}" =~ ^[a-zA-Z][a-zA-Z0-9_-]*$ ]]; then
            echo "::error::Invalid project name. Must start with a letter and contain only letters, numbers, hyphens, and underscores."
            exit 1
          fi

      - name: Set up Python ${{ inputs.python_version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python_version }}

      - name: Create Jupyter project
        run: |
          mkdir ${{ inputs.project_name }}
          cd ${{ inputs.project_name }}
          
          mkdir -p notebooks data/raw data/processed reports src
          
          cat > requirements.txt << 'EOF'
          jupyter>=1.0.0
          jupyterlab>=4.0.0
          notebook>=7.0.0
          ipykernel>=6.25.0
          numpy>=1.24.0
          pandas>=2.0.0
          matplotlib>=3.7.0
          seaborn>=0.12.0
          scikit-learn>=1.3.0
          plotly>=5.18.0
          openpyxl>=3.1.0
          python-dotenv>=1.0.0
          EOF
          
          cat > requirements-dev.txt << 'EOF'
          ruff>=0.3.0
          nbstripout>=0.6.0
          EOF
          
          cat > src/__init__.py << 'EOF'
          EOF
          
          cat > src/utils.py << 'EOF'
          """Common utilities for notebooks."""
          import pandas as pd
          import matplotlib.pyplot as plt
          import seaborn as sns
          from pathlib import Path
          
          PROJECT_ROOT = Path(__file__).parent.parent
          DATA_RAW = PROJECT_ROOT / "data" / "raw"
          DATA_PROCESSED = PROJECT_ROOT / "data" / "processed"
          REPORTS = PROJECT_ROOT / "reports"
          
          
          def setup_plotting():
              """Setup matplotlib and seaborn defaults."""
              plt.style.use('seaborn-v0_8-whitegrid')
              sns.set_palette("husl")
              plt.rcParams['figure.figsize'] = (10, 6)
              plt.rcParams['figure.dpi'] = 100
          
          
          def load_data(filename: str, raw: bool = True) -> pd.DataFrame:
              """Load data from data directory."""
              data_dir = DATA_RAW if raw else DATA_PROCESSED
              filepath = data_dir / filename
              
              if filepath.suffix == '.csv':
                  return pd.read_csv(filepath)
              elif filepath.suffix in ['.xlsx', '.xls']:
                  return pd.read_excel(filepath)
              elif filepath.suffix == '.parquet':
                  return pd.read_parquet(filepath)
              else:
                  raise ValueError(f"Unsupported file format: {filepath.suffix}")
          
          
          def save_data(df: pd.DataFrame, filename: str, processed: bool = True) -> None:
              """Save data to data directory."""
              data_dir = DATA_PROCESSED if processed else DATA_RAW
              filepath = data_dir / filename
              
              if filepath.suffix == '.csv':
                  df.to_csv(filepath, index=False)
              elif filepath.suffix in ['.xlsx', '.xls']:
                  df.to_excel(filepath, index=False)
              elif filepath.suffix == '.parquet':
                  df.to_parquet(filepath, index=False)
              else:
                  raise ValueError(f"Unsupported file format: {filepath.suffix}")
          EOF
          
          touch data/raw/.gitkeep
          touch data/processed/.gitkeep
          touch reports/.gitkeep
          
          cat > notebooks/01_data_exploration.ipynb << 'EOF'
          {
           "cells": [
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["# Data Exploration\n", "\n", "This notebook explores and analyzes the dataset."]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["import sys\n", "sys.path.append('..')\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "from src.utils import setup_plotting, DATA_RAW, DATA_PROCESSED\n", "\n", "setup_plotting()\n", "\n", "print(f\"NumPy: {np.__version__}\")\n", "print(f\"Pandas: {pd.__version__}\")"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Load Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Load your data here\n", "# df = pd.read_csv(DATA_RAW / 'your_data.csv')\n", "# df.head()"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Data Overview"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Explore your data\n", "# df.info()\n", "# df.describe()"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Visualizations"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Create visualizations"]
            }
           ],
           "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "${{ inputs.python_version }}"}
           },
           "nbformat": 4,
           "nbformat_minor": 4
          }
          EOF
          
          cat > notebooks/02_data_cleaning.ipynb << 'EOF'
          {
           "cells": [
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["# Data Cleaning\n", "\n", "This notebook handles data cleaning and preprocessing."]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["import sys\n", "sys.path.append('..')\n", "\n", "import numpy as np\n", "import pandas as pd\n", "\n", "from src.utils import load_data, save_data, DATA_RAW, DATA_PROCESSED"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Load Raw Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# df = load_data('your_data.csv', raw=True)"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Clean Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Handle missing values, duplicates, data types, etc."]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Save Processed Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# save_data(df_cleaned, 'cleaned_data.csv', processed=True)"]
            }
           ],
           "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "${{ inputs.python_version }}"}
           },
           "nbformat": 4,
           "nbformat_minor": 4
          }
          EOF
          
          cat > notebooks/03_analysis.ipynb << 'EOF'
          {
           "cells": [
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["# Analysis\n", "\n", "Main analysis notebook."]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["import sys\n", "sys.path.append('..')\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.model_selection import train_test_split\n", "\n", "from src.utils import setup_plotting, load_data\n", "\n", "setup_plotting()"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Load Processed Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# df = load_data('cleaned_data.csv', raw=False)"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Analysis"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Your analysis here"]
            }
           ],
           "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "${{ inputs.python_version }}"}
           },
           "nbformat": 4,
           "nbformat_minor": 4
          }
          EOF
          
          cat > .gitignore << 'EOF'
          # Python
          __pycache__/
          *.py[cod]
          *$py.class
          *.so
          .Python
          build/
          dist/
          *.egg-info/
          
          # Virtual Environment
          venv/
          .venv/
          ENV/
          
          # Jupyter
          .ipynb_checkpoints/
          
          # Data
          data/raw/*
          data/processed/*
          !data/raw/.gitkeep
          !data/processed/.gitkeep
          
          # Reports
          reports/*
          !reports/.gitkeep
          
          # Environment
          .env
          .env.local
          
          # IDE
          .idea/
          .vscode/
          *.swp
          *.swo
          
          # OS
          .DS_Store
          Thumbs.db
          EOF
          
          cat > README.md << 'EOF'
          # ${{ inputs.project_name }}
          
          A Jupyter data analysis project.
          
          ## Tech Stack
          
          - Python: ${{ inputs.python_version }}
          - Jupyter Lab
          
          ## Quick Start
          
          ```bash
          # Create virtual environment
          python -m venv venv
          source venv/bin/activate  # On Windows: venv\Scripts\activate
          
          # Install dependencies
          pip install -r requirements.txt
          pip install -r requirements-dev.txt  # For development
          
          # Start Jupyter Lab
          jupyter lab
          ```
          
          ## Project Structure
          
          ```
          ${{ inputs.project_name }}/
          ├── notebooks/
          │   ├── 01_data_exploration.ipynb
          │   ├── 02_data_cleaning.ipynb
          │   └── 03_analysis.ipynb
          ├── src/
          │   └── utils.py
          ├── data/
          │   ├── raw/
          │   └── processed/
          ├── reports/
          └── README.md
          ```
          
          ## Workflow
          
          1. Place raw data in `data/raw/`
          2. Run `01_data_exploration.ipynb` to understand the data
          3. Run `02_data_cleaning.ipynb` to clean and preprocess
          4. Run `03_analysis.ipynb` for analysis
          5. Export reports to `reports/`
          EOF

      - name: Create repo and push
        env:
          GH_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          cd ${{ inputs.project_name }}
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"
          git init
          git add .
          git commit -m "Initial Jupyter data analysis project"
          gh repo create ${{ inputs.project_name }} --${{ inputs.repo_visibility }} --source . --push
          echo "✅ Done: https://github.com/${{ github.repository_owner }}/${{ inputs.project_name }}"
