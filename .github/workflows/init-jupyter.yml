name: Init Jupyter Project

on:
  workflow_dispatch:
    inputs:
      project_name:
        description: 'Project Name (will be used as new repository name)'
        required: true
        type: string

      python_version:
        description: 'Python Version'
        required: true
        type: choice
        options:
          - '3.10'
          - '3.11'
          - '3.12'
        default: '3.11'

      repo_visibility:
        description: 'Repository Visibility'
        required: true
        type: choice
        options:
          - 'public'
          - 'private'
        default: 'public'

jobs:
  create-jupyter:
    runs-on: ubuntu-latest
    steps:
      - name: Validate project name
        run: |
          if [[ ! "${{ inputs.project_name }}" =~ ^[a-zA-Z][a-zA-Z0-9_]*$ ]]; then
            echo "::error::Invalid project name. Must start with a letter and contain only letters, numbers, and underscores."
            exit 1
          fi

      # Checkout this repo to get templates
      - name: Checkout templates
        uses: actions/checkout@v4
        with:
          path: templates-repo

      - name: Set up Python ${{ inputs.python_version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python_version }}

      - name: Create Jupyter project
        run: |
          mkdir ${{ inputs.project_name }}
          cd ${{ inputs.project_name }}
          
          # Create directory structure
          mkdir -p notebooks data/raw data/processed reports src
          
          # Create requirements files
          cat > requirements.txt << 'EOF'
          jupyter>=1.0.0
          jupyterlab>=4.0.0
          notebook>=7.0.0
          ipykernel>=6.25.0
          numpy>=1.24.0
          pandas>=2.0.0
          matplotlib>=3.7.0
          seaborn>=0.12.0
          scikit-learn>=1.3.0
          plotly>=5.18.0
          openpyxl>=3.1.0
          python-dotenv>=1.0.0
          EOF
          
          cat > requirements-dev.txt << 'EOF'
          ruff>=0.3.0
          nbstripout>=0.6.0
          EOF
          
          # Create source files
          touch src/__init__.py
          
          cat > src/utils.py << 'PYEOF'
          """Common utilities for notebooks."""
          import pandas as pd
          import matplotlib.pyplot as plt
          import seaborn as sns
          from pathlib import Path

          PROJECT_ROOT = Path(__file__).parent.parent
          DATA_RAW = PROJECT_ROOT / "data" / "raw"
          DATA_PROCESSED = PROJECT_ROOT / "data" / "processed"
          REPORTS = PROJECT_ROOT / "reports"


          def setup_plotting():
              """Setup matplotlib and seaborn defaults."""
              plt.style.use('seaborn-v0_8-whitegrid')
              sns.set_palette("husl")
              plt.rcParams['figure.figsize'] = (10, 6)
              plt.rcParams['figure.dpi'] = 100


          def load_data(filename: str, raw: bool = True) -> pd.DataFrame:
              """Load data from data directory."""
              data_dir = DATA_RAW if raw else DATA_PROCESSED
              filepath = data_dir / filename
              
              if filepath.suffix == '.csv':
                  return pd.read_csv(filepath)
              elif filepath.suffix in ['.xlsx', '.xls']:
                  return pd.read_excel(filepath)
              elif filepath.suffix == '.parquet':
                  return pd.read_parquet(filepath)
              else:
                  raise ValueError(f"Unsupported file format: {filepath.suffix}")


          def save_data(df: pd.DataFrame, filename: str, processed: bool = True) -> None:
              """Save data to data directory."""
              data_dir = DATA_PROCESSED if processed else DATA_RAW
              filepath = data_dir / filename
              
              if filepath.suffix == '.csv':
                  df.to_csv(filepath, index=False)
              elif filepath.suffix in ['.xlsx', '.xls']:
                  df.to_excel(filepath, index=False)
              elif filepath.suffix == '.parquet':
                  df.to_parquet(filepath, index=False)
              else:
                  raise ValueError(f"Unsupported file format: {filepath.suffix}")
          PYEOF
          
          # Create .gitkeep files
          touch data/raw/.gitkeep
          touch data/processed/.gitkeep
          touch reports/.gitkeep
          
          # Create notebooks
          cat > notebooks/01_data_exploration.ipynb << 'EOF'
          {
           "cells": [
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["# Data Exploration\n", "\n", "This notebook explores and analyzes the dataset."]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["import sys\n", "sys.path.append('..')\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "from src.utils import setup_plotting, DATA_RAW, DATA_PROCESSED\n", "\n", "setup_plotting()\n", "\n", "print(f\"NumPy: {np.__version__}\")\n", "print(f\"Pandas: {pd.__version__}\")"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Load Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Load your data here\n", "# df = pd.read_csv(DATA_RAW / 'your_data.csv')\n", "# df.head()"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Data Overview"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Explore your data\n", "# df.info()\n", "# df.describe()"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Visualizations"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Create visualizations"]
            }
           ],
           "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "${{ inputs.python_version }}"}
           },
           "nbformat": 4,
           "nbformat_minor": 4
          }
          EOF
          
          cat > notebooks/02_data_cleaning.ipynb << 'EOF'
          {
           "cells": [
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["# Data Cleaning\n", "\n", "This notebook handles data cleaning and preprocessing."]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["import sys\n", "sys.path.append('..')\n", "\n", "import numpy as np\n", "import pandas as pd\n", "\n", "from src.utils import load_data, save_data, DATA_RAW, DATA_PROCESSED"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Load Raw Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# df = load_data('your_data.csv', raw=True)"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Clean Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Handle missing values, duplicates, data types, etc."]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Save Processed Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# save_data(df_cleaned, 'cleaned_data.csv', processed=True)"]
            }
           ],
           "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "${{ inputs.python_version }}"}
           },
           "nbformat": 4,
           "nbformat_minor": 4
          }
          EOF
          
          cat > notebooks/03_analysis.ipynb << 'EOF'
          {
           "cells": [
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["# Analysis\n", "\n", "Main analysis notebook."]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["import sys\n", "sys.path.append('..')\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.model_selection import train_test_split\n", "\n", "from src.utils import setup_plotting, load_data\n", "\n", "setup_plotting()"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Load Processed Data"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# df = load_data('cleaned_data.csv', raw=False)"]
            },
            {
             "cell_type": "markdown",
             "metadata": {},
             "source": ["## Analysis"]
            },
            {
             "cell_type": "code",
             "execution_count": null,
             "metadata": {},
             "outputs": [],
             "source": ["# Your analysis here"]
            }
           ],
           "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "${{ inputs.python_version }}"}
           },
           "nbformat": 4,
           "nbformat_minor": 4
          }
          EOF
          
          # Create release.yaml
          cat > release.yaml << 'EOF'
          version: "0.1.0"
          EOF

      - name: Copy template files
        run: |
          cd ${{ inputs.project_name }}
          
          # Copy shared files (CI/CD workflow)
          mkdir -p .github/workflows
          cp ../templates-repo/templates/shared/.github/workflows/docker-build-push.yml .github/workflows/
          
          # Copy Jupyter-specific files
          cp ../templates-repo/templates/jupyter/.gitignore .
          
          # Process Dockerfile template
          sed -e 's/{{PYTHON_VERSION}}/${{ inputs.python_version }}/g' \
              ../templates-repo/templates/jupyter/Dockerfile.template > Dockerfile

      - name: Create README
        run: |
          cd ${{ inputs.project_name }}
          cat > README.md << 'EOF'
          # ${{ inputs.project_name }}
          
          A Jupyter data analysis project.
          
          ## Tech Stack
          
          - Python: ${{ inputs.python_version }}
          - Jupyter Lab
          
          ## Quick Start
          
          ```bash
          # Create virtual environment
          python -m venv venv
          source venv/bin/activate  # On Windows: venv\Scripts\activate
          
          # Install dependencies
          pip install -r requirements.txt
          pip install -r requirements-dev.txt  # For development
          
          # Start Jupyter Lab
          jupyter lab
          ```
          
          ## Docker
          
          ```bash
          # Build image
          docker build -t ${{ inputs.project_name }}:latest .
          
          # Run Jupyter Lab in container
          docker run -p 8888:8888 -v $(pwd)/data:/app/data ${{ inputs.project_name }}:latest
          ```
          
          Then visit http://localhost:8888
          
          ## CI/CD
          
          This project includes a GitHub Action workflow for automatic Docker image builds:
          - Triggers on push to main/master branch
          - Can be manually triggered via workflow_dispatch
          - Pushes images to DockerHub
          - Runs Trivy security scan
          
          ### Required GitHub Secrets
          
          | Secret | Description |
          |--------|-------------|
          | `DOCKERHUB_USERNAME` | Your DockerHub username |
          | `DOCKERHUB_TOKEN` | Your DockerHub access token |
          
          ## Project Structure
          
          ```
          ${{ inputs.project_name }}/
          â”œâ”€â”€ notebooks/
          â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
          â”‚   â”œâ”€â”€ 02_data_cleaning.ipynb
          â”‚   â””â”€â”€ 03_analysis.ipynb
          â”œâ”€â”€ src/
          â”‚   â””â”€â”€ utils.py
          â”œâ”€â”€ data/
          â”‚   â”œâ”€â”€ raw/
          â”‚   â””â”€â”€ processed/
          â”œâ”€â”€ reports/
          â”œâ”€â”€ .github/workflows/
          â”‚   â””â”€â”€ docker-build-push.yml
          â”œâ”€â”€ Dockerfile
          â””â”€â”€ README.md
          ```
          
          ## Workflow
          
          1. Place raw data in `data/raw/`
          2. Run `01_data_exploration.ipynb` to understand the data
          3. Run `02_data_cleaning.ipynb` to clean and preprocess
          4. Run `03_analysis.ipynb` for analysis
          5. Export reports to `reports/`
          EOF

      - name: Create repo and push
        env:
          GH_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          cd ${{ inputs.project_name }}
          git config --global user.name "${{ github.actor }}"
          git config --global user.email "${{ github.actor }}@users.noreply.github.com"
          git init
          git add .
          git commit -m "Initial Jupyter data analysis project"
          gh repo create ${{ inputs.project_name }} --${{ inputs.repo_visibility }} --source . --push
          echo "âœ… Repo created: https://github.com/${{ github.repository_owner }}/${{ inputs.project_name }}"

      - name: Create poc branch
        env:
          GH_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          cd ${{ inputs.project_name }}
          git remote set-url origin https://x-access-token:${GH_TOKEN}@github.com/${{ github.repository_owner }}/${{ inputs.project_name }}.git
          git checkout -b poc
          git push -u origin poc
          git checkout main
          echo "âœ… Branch 'poc' created"

      - name: Set secrets for new repo
        env:
          GH_TOKEN: ${{ secrets.PAT_TOKEN }}
        run: |
          REPO="${{ github.repository_owner }}/${{ inputs.project_name }}"
          echo "ðŸ” Setting secrets for ${REPO}..."
          echo "${{ secrets.DOCKERHUB_USERNAME }}" | gh secret set DOCKERHUB_USERNAME --repo ${REPO}
          echo "${{ secrets.DOCKERHUB_TOKEN }}" | gh secret set DOCKERHUB_TOKEN --repo ${REPO}
          echo "âœ… Secrets configured"
